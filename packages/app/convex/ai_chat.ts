import { ai_chat_HARDCODED_ORG_ID, ai_chat_HARDCODED_PROJECT_ID } from "../shared/shared-utils.ts";
import { type ai_chat_AiSdkUiMessage } from "../src/lib/ai-chat.ts";
import { get_id_generator, math_clamp } from "../src/lib/utils.ts";
import { query, mutation, httpAction, internalMutation, type ActionCtx } from "./_generated/server.js";
import { api } from "./_generated/api.js";
import { paginationOptsValidator, type RouteSpec } from "convex/server";
import { v } from "convex/values";
import { openai } from "@ai-sdk/openai";
import {
	streamText,
	tool,
	smoothStream,
	createUIMessageStream,
	createUIMessageStreamResponse,
	stepCountIs,
	convertToModelMessages,
	validateUIMessages,
	TypeValidationError,
	type Tool,
	type ModelMessage,
} from "ai";
import { z } from "zod";
import { type api_schemas_BuildResponseSpecFromHandler, type api_schemas_Main_Path } from "../shared/api-schemas.ts";
import {
	server_convex_get_user_fallback_to_anonymous,
	server_request_json_parse_and_validate,
} from "../server/server-utils.ts";
import type { app_convex_Doc, app_convex_Id } from "../src/lib/app-convex-client.ts";
import {
	ai_chat_tool_create_list_pages,
	ai_chat_tool_create_read_page,
	ai_chat_tool_create_glob_pages,
	ai_chat_tool_create_grep_pages,
	ai_chat_tool_create_text_search_pages,
	ai_chat_tool_create_write_page,
	ai_chat_tool_create_edit_page,
} from "../server/server-ai-tools.ts";
import app_convex_schema from "./schema.ts";
import type { RouterForConvexModules } from "./http.ts";
import { Result } from "../shared/errors-as-values-utils.ts";

/**
 * This function exists only becase is not possible to define a type specific enough to make ts happy when using
 * the data saved in convex as {@link ai_chat_AiSdkUiMessage}. However, {@link ai_chat_AiSdkUiMessage} is the type that is saved in convex.
 */
function convert_convex_db_message_to_aisdk_ui_message(message: app_convex_Doc<"messages">): ai_chat_AiSdkUiMessage {
	return {
		id: message._id,
		role: message.content.role,
		parts: message.content.parts as ai_chat_AiSdkUiMessage["parts"],
		metadata: message.content.metadata as ai_chat_AiSdkUiMessage["metadata"],
	};
}

export const threads_list = query({
	args: {
		paginationOpts: paginationOptsValidator,
		archived: v.optional(v.boolean()),
	},
	handler: async (ctx, args) => {
		const numItems = math_clamp(args.paginationOpts.numItems ?? 100, 1, 100);
		const archived = args.archived ?? false;

		const threads_query = ctx.db
			.query("threads")
			.withIndex("by_workspace_project_archived", (q) =>
				q
					.eq("workspace_id", ai_chat_HARDCODED_ORG_ID)
					.eq("project_id", ai_chat_HARDCODED_PROJECT_ID)
					.eq("archived", archived),
			);

		const result = await threads_query.order("desc").paginate({
			...args.paginationOpts,
			numItems,
		});

		return result;
	},
});

/**
 * Query to get a single thread by ID
 */
export const thread_get = query({
	args: {
		/**
		 * Can be a temporary ID generated by Assistant UI
		 **/
		threadId: v.string(),
	},
	handler: async (ctx, args) => {
		const id_normalized = ctx.db.normalizeId("threads", args.threadId);

		if (!id_normalized) {
			return null;
		}

		const thread = await ctx.db.get("threads", id_normalized);

		// Verify the thread belongs to the current workspace
		if (thread && thread.workspace_id !== ai_chat_HARDCODED_ORG_ID) {
			return null;
		}

		return thread;
	},
});

/**
 * Mutation to create a new thread
 */
export const thread_create = mutation({
	args: {
		title: v.optional(v.string()),
		lastMessageAt: v.number(), // timestamp in milliseconds
		metadata: v.optional(v.any()),
		externalId: v.optional(v.union(v.string())),
	},
	handler: async (ctx, args) => {
		const created_by = await server_convex_get_user_fallback_to_anonymous(ctx);

		const now = Date.now();

		const thread_id = await ctx.db.insert("threads", {
			title: args.title ?? null,
			last_message_at: args.lastMessageAt,
			archived: false,
			workspace_id: ai_chat_HARDCODED_ORG_ID,
			created_by: created_by.name,
			updated_by: created_by.name,
			updated_at: now,
			external_id: args.externalId ?? null,
			project_id: ai_chat_HARDCODED_PROJECT_ID,
			starred: false,
		});

		return {
			thread_id,
		};
	},
});

/**
 * Mutation to update thread details
 */
export const thread_update = mutation({
	args: {
		threadId: v.string(),
		title: v.optional(v.union(v.string(), v.null())),
		isArchived: v.optional(v.boolean()),
		starred: v.optional(v.boolean()),
	},
	handler: async (ctx, args) => {
		const updated_by = await server_convex_get_user_fallback_to_anonymous(ctx);

		const threadId = ctx.db.normalizeId("threads", args.threadId);
		if (!threadId) {
			return Result({ _nay: { message: "Invalid threadId" } });
		}

		await ctx.db.patch(
			"threads",
			threadId,
			Object.assign(
				{
					updated_by: updated_by.name,
					updated_at: Date.now(),
				},
				args.title !== undefined
					? {
							title: args.title,
						}
					: {},
				args.isArchived !== undefined
					? {
							archived: args.isArchived,
						}
					: {},
				args.starred !== undefined
					? {
							starred: args.starred,
						}
					: {},
			),
		);
	},
});

/**
 * Mutation to archive/unarchive a thread
 */
export const thread_archive = mutation({
	args: {
		threadId: v.id("threads"),
	},
	handler: async (ctx, args) => {
		const updated_by = await server_convex_get_user_fallback_to_anonymous(ctx);

		const now = Date.now();

		await ctx.db.patch("threads", args.threadId, {
			archived: true,
			updated_by: updated_by.name,
			updated_at: now,
		});
	},
});

/**
 * Query to list messages in a thread
 */
export const thread_messages_list = query({
	args: {
		threadId: v.string(),
		order: v.optional(v.union(v.literal("asc"), v.literal("desc"))),
		query: v.optional(
			v.object({
				format: v.optional(v.string()),
			}),
		),
	},
	handler: async (ctx, args) => {
		const thread_id_normalized = ctx.db.normalizeId("threads", args.threadId);

		if (!thread_id_normalized) {
			return null;
		}

		const messagesRaw = await ctx.db
			.query("messages")
			.withIndex("by_thread", (q) => q.eq("thread_id", thread_id_normalized))
			.order(args.order ?? "desc")
			.collect();

		const format = args.query?.format;
		const messagesFiltered = format ? messagesRaw.filter((message) => message.format === format) : messagesRaw;

		// De-duplicate by client-generated message id (or fallback to _id).
		// This prevents assistant-ui crashes if duplicates were inserted previously.
		const seen = new Set<string>();
		const messages = messagesFiltered.filter((message) => {
			const id = message.client_generated_message_id ?? message._id;
			if (seen.has(id)) return false;
			seen.add(id);
			return true;
		});

		return { messages };
	},
});

/**
 * Resolve a Convex message id from an Assistant UI / AI SDK client-generated message id.
 *
 * Used to derive a stable branch point when the client cannot provide `parentId` / `messageId`
 * during regenerate flows.
 */
export const message_id_get_by_client_generated_message_id = query({
	args: {
		threadId: v.id("threads"),
		clientGeneratedMessageId: v.string(),
	},
	returns: v.union(v.id("messages"), v.null()),
	handler: async (ctx, args) => {
		const existingWithClientId = await ctx.db
			.query("messages")
			.withIndex("by_client_generated_message_id", (q) =>
				q.eq("client_generated_message_id", args.clientGeneratedMessageId),
			)
			.order("asc")
			.collect();

		const existingForThread = existingWithClientId.find((doc) => doc.thread_id === args.threadId);
		return existingForThread?._id ?? null;
	},
});

/**
 * Mutation to add a message to a thread
 */
export const thread_messages_add = mutation({
	args: {
		threadId: v.id("threads"),
		parentId: v.union(v.id("messages"), v.null()),
		format: v.string(),
		content: v.object({
			role: app_convex_schema.tables.messages.validator.fields.content.fields.role,
			parts: app_convex_schema.tables.messages.validator.fields.content.fields.parts,
			metadata: app_convex_schema.tables.messages.validator.fields.content.fields.metadata,
		}),
	},
	handler: async (ctx, args) => {
		const created_by = await server_convex_get_user_fallback_to_anonymous(ctx);

		const now = Date.now();

		// Insert the message
		const message_id = await ctx.db.insert("messages", {
			parent_id: args.parentId,
			thread_id: args.threadId,
			created_by: created_by.name,
			updated_by: created_by.name,
			updated_at: now,
			format: args.format,
			height: 1,
			content: args.content,
		});

		// Update the thread's lastMessageAt timestamp
		try {
			await ctx.db.patch("threads", args.threadId, {
				last_message_at: now,
				updated_at: now,
				updated_by: created_by.name,
			});
		} catch (error) {
			console.error("Failed to update thread when adding message", error);
		}

		return { message_id };
	},
});

/**
 * Mutation to add multiple messages to a thread.
 */
export const thread_messages_add_many = mutation({
	args: {
		threadId: v.id("threads"),
		parentId: v.union(v.id("messages"), v.null()),
		messages: v.array(
			v.object({
				id: v.string(),
				format: v.string(),
				content: v.object({
					role: app_convex_schema.tables.messages.validator.fields.content.fields.role,
					parts: app_convex_schema.tables.messages.validator.fields.content.fields.parts,
					metadata: app_convex_schema.tables.messages.validator.fields.content.fields.metadata,
				}),
			}),
		),
	},
	returns: v.object({
		messageIds: v.array(v.id("messages")),
		idMap: v.record(v.string(), v.id("messages")),
	}),
	handler: async (ctx, args) => {
		const created_by = await server_convex_get_user_fallback_to_anonymous(ctx);
		const now = Date.now();

		const messageIds: Array<app_convex_Id<"messages">> = [];
		const idMap: Record<string, app_convex_Id<"messages">> = {};

		let nextParentId = args.parentId;
		let skippedExistingCount = 0;
		let insertedCount = 0;

		for (const message of args.messages) {
			const existingWithClientId = await ctx.db
				.query("messages")
				.withIndex("by_client_generated_message_id", (q) => q.eq("client_generated_message_id", message.id))
				.order("asc")
				.collect();

			const existingForThread = existingWithClientId.find((doc) => doc.thread_id === args.threadId);
			if (existingForThread) {
				skippedExistingCount += 1;
				messageIds.push(existingForThread._id);
				idMap[message.id] = existingForThread._id;
				nextParentId = existingForThread._id;
				continue;
			}

			const messageId = await ctx.db.insert("messages", {
				parent_id: nextParentId,
				thread_id: args.threadId,
				created_by: created_by.name,
				updated_by: created_by.name,
				updated_at: now,
				format: message.format,
				height: 1,
				client_generated_message_id: message.id,
				content: message.content,
			});

			insertedCount += 1;
			messageIds.push(messageId);
			idMap[message.id] = messageId;
			nextParentId = messageId;
		}

		if (messageIds.length > 0) {
			try {
				await ctx.db.patch("threads", args.threadId, {
					last_message_at: now,
					updated_at: now,
					updated_by: created_by.name,
				});
			} catch (error) {
				console.error("Failed to update thread when adding messages", error);
			}
		}

		return { messageIds, idMap };
	},
});

export const upsert_ai_pending_edit = internalMutation({
	args: {
		workspaceId: v.string(),
		projectId: v.string(),
		threadId: v.string(),
		pageId: v.string(),
		baseContent: v.string(),
		modifiedContent: v.string(),
	},
	returns: v.null(),
	handler: async (ctx, args) => {
		const user = await server_convex_get_user_fallback_to_anonymous(ctx);
		const now = Date.now();
		const existing = await ctx.db
			.query("ai_chat_pending_edits")
			.withIndex("by_user_thread_page", (q) =>
				q.eq("user_id", user.id).eq("thread_id", args.threadId).eq("page_id", args.pageId),
			)
			.first();

		if (!existing) {
			await ctx.db.insert("ai_chat_pending_edits", {
				workspace_id: args.workspaceId,
				project_id: args.projectId,
				user_id: user.id,
				thread_id: args.threadId,
				page_id: args.pageId,
				base_content: args.baseContent,
				modified_content: args.modifiedContent,
				updated_at: now,
			});
		} else {
			await ctx.db.patch("ai_chat_pending_edits", existing._id, {
				modified_content: args.modifiedContent,
				updated_at: now,
			});
		}
		return null;
	},
});

export const get_ai_pending_edit = query({
	args: {
		pageId: v.string(),
		threadId: v.string(),
	},
	returns: v.union(v.object({ page_id: v.string(), base_content: v.string(), modified_content: v.string() }), v.null()),
	handler: async (ctx, args) => {
		const user = await server_convex_get_user_fallback_to_anonymous(ctx);
		const pending = await ctx.db
			.query("ai_chat_pending_edits")
			.withIndex("by_user_thread_page", (q) =>
				q.eq("user_id", user.id).eq("thread_id", args.threadId).eq("page_id", args.pageId),
			)
			.first();
		if (!pending) return null;
		return {
			page_id: pending.page_id,
			base_content: pending.base_content,
			modified_content: pending.modified_content,
		};
	},
});

export function ai_chat_http_routes(router: RouterForConvexModules) {
	return {
		...((/* iife */ path = "/api/chat" as const satisfies api_schemas_Main_Path) => ({
			[path]: {
				...((/* iife */ method = "POST" as const satisfies RouteSpec["method"]) => ({
					[method]: ((/* iife */) => {
						/**
						 * See {@link PrepareSendMessagesRequest}.
						 *
						 * See {@link AssistantChatTransport.prepareSendMessagesRequest}.
						 **/
						const bodyValidator = z.object({
							// AI SDK fields
							id: z.string(),
							messages: z.array(z.any()),
							trigger: z.enum(["submit-message", "regenerate-message"]),
							messageId: z.string().optional(),

							// Assistant UI fields
							system: z.string().optional(),
							tools: z.record(z.string(), z.unknown()),

							// Custom fields
							threadId: z.string().optional(),
							// `null` means "root" (explicitly do not append to latest message).
							parentId: z.union([z.string(), z.null()]).optional(),
							clientThreadId: z.string().optional(),
						});

						type SearchParams = never;
						type PathParams = never;
						type Headers = Record<string, string>;
						type Body = z.infer<typeof bodyValidator>;

						const handler = async (ctx: ActionCtx, request: Request) => {
							try {
								const requestParseResult = await server_request_json_parse_and_validate(request, bodyValidator);

								if (requestParseResult._nay) {
									return {
										status: 400,
										body: requestParseResult._nay,
									} as const;
								}

								const body = requestParseResult._yay;

								// Validate messages from request
								if (!Array.isArray(body.messages)) {
									return {
										status: 400,
										body: {
											message: "Invalid messages format",
										},
									} as const;
								}

								let threadId = body.threadId ?? null;
								let createdThreadId: string | null = null;

								if (threadId) {
									const existingThread: app_convex_Doc<"threads"> | null = await ctx.runQuery(api.ai_chat.thread_get, {
										threadId,
									});
									if (!existingThread) {
										return {
											status: 404,
											body: {
												message: "Thread not found",
											},
										} as const;
									}

									threadId = existingThread._id;
								} else {
									const created = await ctx.runMutation(api.ai_chat.thread_create, {
										lastMessageAt: Date.now(),
										// Store the optimistic client thread id on the persisted thread.
										// This lets the frontend dedupe the optimistic entry as soon as the
										// thread appears in `threads_list`, even if the SSE `data-thread-id`
										// mapping arrives slightly later.
										externalId: body.clientThreadId ?? undefined,
									});
									threadId = created.thread_id;
									createdThreadId = created.thread_id;
								}

								const requestMessages = body.messages as ai_chat_AiSdkUiMessage[];

								// TODO(ai-chat): "Persist-first" message flow (user + assistant)
								// - Persist the incoming *user* message immediately (before streaming) to allocate a Convex message _id.
								// - Allocate a placeholder *assistant* message doc up front as well (child of the user message).
								// - Emit a transient stream part (e.g. `data-message-ids`) with:
								//   - client message id(s) (from AI SDK UIMessage) -> convex message _id mapping
								//   - optionally also include a `persisted: true` / "barrier" so the client can drop optimistic messages.
								// - Once this is in place, we should be able to remove (or at least stop depending on) the
								//   `messages.client_generated_message_id` persistence/dedupe path.
								let messages: ModelMessage[] = [];
								let uiMessages: ai_chat_AiSdkUiMessage[] = [];
								let resolvedParentId: app_convex_Id<"messages"> | null = null;

								do {
									if (body.parentId) {
										resolvedParentId = body.parentId as app_convex_Id<"messages">;
									} else if (body.parentId === null) {
										resolvedParentId = null;
									} else if (body.trigger === "regenerate-message") {
										const lastClientMessageId =
											Array.isArray(body.messages) && typeof (body.messages.at(-1) as any)?.id === "string"
												? ((body.messages.at(-1) as any).id as string)
												: null;

										if (lastClientMessageId) {
											// TODO(ai-chat): Remove reliance on stored `client_generated_message_id` for regenerate.
											// If we persist user/assistant messages up front and propagate Convex ids to the client,
											// regenerate should always have a stable Convex parent id available and this fallback
											// query can go away.
											const resolvedFromClientId: app_convex_Id<"messages"> | null = await ctx.runQuery(
												api.ai_chat.message_id_get_by_client_generated_message_id,
												{
													threadId: threadId as app_convex_Id<"threads">,
													clientGeneratedMessageId: lastClientMessageId,
												},
											);
											resolvedParentId = resolvedFromClientId;
										}
									}

									if (threadId) {
										const threadMessagesResult = await ctx.runQuery(api.ai_chat.thread_messages_list, {
											threadId: threadId as app_convex_Id<"threads">,
											order: "asc",
										});

										if (!threadMessagesResult) {
											uiMessages = requestMessages;
											break;
										}

										// If the client didn't provide a parent id (race condition with initial thread hydration),
										// fall back to appending to the latest message in the thread.
										if (!resolvedParentId && body.trigger !== "regenerate-message" && body.parentId !== null) {
											resolvedParentId = threadMessagesResult.messages.at(-1)?._id ?? null;
										}

										const messagesMap = new Map<string, app_convex_Doc<"messages">>(
											threadMessagesResult.messages.map((msg) => [msg._id, msg]),
										);

										const reconstructedMessages: app_convex_Doc<"messages">[] = [];

										let nextParentId = resolvedParentId;

										while (nextParentId) {
											const parentMessage = messagesMap.get(nextParentId);
											if (parentMessage) {
												// the messages has to be from the oldest to the newest
												reconstructedMessages.unshift(parentMessage);
												nextParentId = parentMessage.parent_id;
											} else {
												console.warn(`Parent message not found: ${nextParentId}`);
												break;
											}
										}

										uiMessages = reconstructedMessages
											.map((msg) => convert_convex_db_message_to_aisdk_ui_message(msg))
											.concat(...requestMessages);
									}
								} while (0);

								const tools = {
									weather: tool({
										description: "Get the weather in a location (in Celsius)",
										inputSchema: z.object({
											location: z.string().describe("The location to get the weather for"),
										}),
										execute: async ({ location }) => ({
											location,
											temperature: "200Â°",
										}),
									}),
									read_page: ai_chat_tool_create_read_page(ctx, { thread_id: threadId ?? "" }),
									list_pages: ai_chat_tool_create_list_pages(ctx),
									glob_pages: ai_chat_tool_create_glob_pages(ctx),
									grep_pages: ai_chat_tool_create_grep_pages(ctx, { thread_id: threadId ?? "" }),
									text_search_pages: ai_chat_tool_create_text_search_pages(ctx, { thread_id: threadId ?? "" }),
									write_page: ai_chat_tool_create_write_page(ctx, { thread_id: threadId ?? "" }),
									edit_page: ai_chat_tool_create_edit_page(ctx, { thread_id: threadId ?? "" }),
								} as const;

								try {
									uiMessages = await validateUIMessages({
										messages: uiMessages,
										tools: tools as Record<string, Tool<unknown, unknown>>,
									});
								} catch (error) {
									if (error instanceof TypeValidationError) {
										console.error("Failed to validate chat messages, falling back to request messages.", error);
										uiMessages = requestMessages;
									} else {
										throw error;
									}
								}

								messages = convertToModelMessages(uiMessages);

								console.info("messages", {
									messages: messages,
									threadId,
								});

								const stream = createUIMessageStream<ai_chat_AiSdkUiMessage>({
									generateId: get_id_generator("ai_message"),
									execute: async ({ writer }) => {
										// TODO(ai-chat): If we allocate Convex message docs up front, emit a transient `data-message-ids`
										// part here (while `writer` is available) so the client can swap optimistic UIMessage ids to
										// Convex ids and/or drop optimistic messages immediately, without persisting client ids in DB.
										if (createdThreadId) {
											writer.write({
												type: "data-thread-id",
												data: {
													threadId: createdThreadId,
												},
												transient: true,
											});
										}

										// Attach the resolved Convex parent id as message metadata so the client can treat
										// the streaming (optimistic) assistant message as part of the parent/child tree.
										//
										// For normal submits, we want the streaming assistant message to be a child of the optimistic
										// user message (client-generated id), so the UI includes the user message in the active branch.
										const requestMessages = body.messages as ai_chat_AiSdkUiMessage[];
										const lastRequestMessage = requestMessages.at(-1) ?? null;
										const parentIdForStreamingMetadata =
											body.trigger === "regenerate-message"
												? (resolvedParentId ?? null)
												: lastRequestMessage?.role === "user"
													? lastRequestMessage.id
													: (resolvedParentId ?? null);
										writer.write({
											type: "message-metadata",
											messageMetadata: {
												convexParentId: parentIdForStreamingMetadata,
											},
										});

										const result1 = streamText({
											model: openai("gpt-5-nano"),
											system: `Either respond directly to the user or use the tools at your disposal.`,
											messages,
											temperature: 0.7,
											maxOutputTokens: 2000,
											abortSignal: request.signal,
											providerOptions: {
												openai: {
													reasoningEffort: "low",
												},
											},
											toolChoice: "auto",
											stopWhen: stepCountIs(5),
											tools,
											experimental_transform: smoothStream({
												delayInMs: 100,
											}),
										});

										writer.merge(result1.toUIMessageStream());

										if (request.signal.aborted) {
											return;
										}

										const response1 = await result1.response;

										if (request.signal.aborted) {
											return;
										}

										const thread = await ctx.runQuery(api.ai_chat.thread_get, { threadId });
										const existingTitle = typeof thread?.title === "string" ? thread.title.trim() : "";

										if (thread && !existingTitle) {
											try {
												if (request.signal.aborted) {
													return;
												}

												const titleMessages = [...messages, ...response1.messages];
												const titleResult = streamText({
													model: openai("gpt-4.1-nano"),
													system:
														"Generate a concise, descriptive title (max 6 words) for this conversation.\n" +
														"The title should capture the main topic or purpose.\n" +
														"Respond with ONLY the title, no quotes or extra text.",
													messages: titleMessages,
													stopWhen: stepCountIs(1),
													temperature: 0.3,
													maxOutputTokens: 50,
													abortSignal: request.signal,
												});

												const reader = titleResult.textStream.getReader();
												let title = "";
												while (true) {
													const { value, done } = await reader.read();
													if (done) {
														break;
													}

													if (value) {
														title += value;
													}
												}

												const trimmedTitle = title.trim();
												if (trimmedTitle) {
													writer.write({
														type: "data-chat-title",
														data: { title: trimmedTitle },
														transient: true,
													});

													await ctx.runMutation(api.ai_chat.thread_update, {
														threadId: thread._id,
														title: trimmedTitle,
													});
												}
											} catch (error: unknown) {
												console.error("Title generation error:", error);
											}
										}
									},
									onError: (error: unknown) => {
										console.error("AI chat stream error:", error);
										return error instanceof Error ? error.message : String(error);
									},
									onFinish: async (result) => {
										try {
											if (request.signal.aborted) {
												return;
											}

											// TODO(ai-chat): Once messages are allocated up front, switch this to patch/update the
											// placeholder docs (user + assistant) rather than inserting via `thread_messages_add_many`.
											// That would make Convex `_id` the canonical message id end-to-end and allow us to drop
											// `client_generated_message_id` (and its index/query) over time.
											const requestMessages = body.messages as ai_chat_AiSdkUiMessage[];
											const responseMessage = result.responseMessage;
											const messagesToPersist = responseMessage
												? requestMessages.concat(responseMessage)
												: requestMessages;

											if (messagesToPersist.length === 0) {
												return;
											}

											const messagesInput = messagesToPersist.map((message) => {
												const parts = (message.parts ?? []).filter((part) => part.type !== "file");
												const content = Object.assign(
													{
														role: message.role,
														parts,
													},
													message.metadata != null ? { metadata: message.metadata } : null,
												);

												return {
													id: message.id,
													format: "ai-sdk/v5",
													content,
												};
											});

											await ctx.runMutation(api.ai_chat.thread_messages_add_many, {
												threadId: threadId as app_convex_Id<"threads">,
												parentId: resolvedParentId,
												messages: messagesInput,
											});
										} catch (error: unknown) {
											console.error("Failed to persist chat messages:", error);
										}
									},
								});

								return {
									status: 200,
									body: stream,
								} as const;
							} catch (error: unknown) {
								console.error("AI chat stream error:", error);

								if (error instanceof Error) {
									return {
										status: 500,
										body: {
											message: error.message,
										},
									} as const;
								}

								return {
									status: 500,
									body: {
										message: "Internal server error",
									},
								} as const;
							}
						};

						router.route({
							path,
							method,
							handler: httpAction(async (ctx, request) => {
								const result = await handler(ctx, request);

								if (result.status === 200) {
									return createUIMessageStreamResponse({
										status: result.status,
										stream: result.body,
									});
								}

								return Response.json(result.body, result);
							}),
						});

						return {} as {
							pathParams: PathParams;
							searchParams: SearchParams;
							headers: Headers;
							body: Body;
							response: api_schemas_BuildResponseSpecFromHandler<typeof handler>;
						};
					})(),
				}))(),
			},
		}))(),

		...((/* iife */ path = "/api/v1/runs/stream" as const satisfies api_schemas_Main_Path) => ({
			[path]: {
				...((/* iife */ method = "POST" as const satisfies RouteSpec["method"]) => ({
					[method]: ((/* iife */) => {
						/**
						 * See {@link PrepareSendMessagesRequest}.
						 *
						 * See {@link AssistantChatTransport.prepareSendMessagesRequest}.
						 **/
						const bodyValidator = z.object({
							thread_id: z.string(),
							assistant_id: z.string(),
							messages: z.array(z.any()),
							response_format: z.string().optional(),
						});

						type SearchParams = never;
						type PathParams = never;
						type Headers = Record<string, string>;
						type Body = z.infer<typeof bodyValidator>;

						const handler = async (ctx: ActionCtx, request: Request) => {
							try {
								const body = await request.json();

								if (body.assistant_id !== "system/thread_title") {
									return {
										status: 400,
										body: {
											message: "Invalid stream ID",
										},
									} as const;
								}

								const messages = body.messages || [];
								const thread_id = body.thread_id;

								// Extract conversation text from messages for title generation
								const conversation_text = messages
									.map((msg: any) =>
										[
											`${msg.role}:`,
											Array.isArray(msg.content) ? msg.content.map((part: any) => part.text).join(" ") : msg.content,
										]
											.filter(Boolean)
											.join(" "),
									)
									.filter(Boolean)
									.join("\n");

								// Generate title using AI with streaming
								const result = streamText({
									model: openai("gpt-4.1-nano"),
									system: `Generate a concise, descriptive title (max 6 words) for this conversation.
										The title should capture the main topic or purpose.
										Respond with ONLY the title, no quotes or extra text.`,
									messages: [
										{
											role: "user",
											content: `Generate a title for this conversation:\n\n${conversation_text}`,
										},
									],
									stopWhen: stepCountIs(1),
									temperature: 0.3,
									maxOutputTokens: 50,
									experimental_transform: smoothStream({
										delayInMs: 100,
									}),
								});

								// Transform the AI stream to properly encode text chunks
								let title = "";

								// Trigger mutation when the stream is finished
								const transform_stream = new TransformStream({
									transform(chunk, controller) {
										title += chunk;
										controller.enqueue(chunk);
									},
									flush: async () => {
										await ctx.runMutation(api.ai_chat.thread_update, {
											threadId: thread_id,
											title,
										});
									},
								});

								// Pipe the AI textStream through the transformer, insprired by ai-sdk's `createTextStreamResponse`
								const stream = result.textStream.pipeThrough(transform_stream).pipeThrough(new TextEncoderStream());

								void result.consumeStream();

								return {
									status: 200,
									body: stream,
								} as const;
							} catch (error: unknown) {
								console.error("Title generation error:", error);

								return {
									status: 500,
									body: {
										message: error instanceof Error ? error.message : "Unknown error",
									},
								} as const;
							}
						};

						router.route({
							path,
							method,
							handler: httpAction(async (ctx, request) => {
								const result = await handler(ctx, request);

								if (result.status === 200) {
									return new Response(result.body, {
										status: result.status,
									});
								}

								return Response.json(result, result);
							}),
						});

						return {} as {
							pathParams: PathParams;
							searchParams: SearchParams;
							headers: Headers;
							body: Body;
							response: api_schemas_BuildResponseSpecFromHandler<typeof handler>;
						};
					})(),
				}))(),
			},
		}))(),
	};
}
