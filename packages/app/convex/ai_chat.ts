import {
	ai_chat_HARDCODED_ORG_ID,
	ai_chat_HARDCODED_PROJECT_ID,
	omit_properties,
	should_never_happen,
} from "../shared/shared-utils.ts";
import { type ai_chat_AiSdk5UiMessage } from "../src/lib/ai-chat.ts";
import { get_id_generator, math_clamp } from "../src/lib/utils.ts";
import { query, mutation, httpAction, internalMutation, type ActionCtx } from "./_generated/server.js";
import { api } from "./_generated/api.js";
import { paginationOptsValidator, type RouteSpec } from "convex/server";
import { doc } from "convex-helpers/validators";
import { v } from "convex/values";
import { openai } from "@ai-sdk/openai";
import {
	streamText,
	tool,
	smoothStream,
	createUIMessageStream,
	createUIMessageStreamResponse,
	consumeStream,
	stepCountIs,
	convertToModelMessages,
	validateUIMessages,
	TypeValidationError,
} from "ai";
import { z } from "zod";
import { type api_schemas_BuildResponseSpecFromHandler, type api_schemas_Main_Path } from "../shared/api-schemas.ts";
import {
	server_convex_get_user_fallback_to_anonymous,
	server_request_json_parse_and_validate,
} from "../server/server-utils.ts";
import type { app_convex_Doc, app_convex_Id } from "../src/lib/app-convex-client.ts";
import {
	ai_chat_tool_create_list_pages,
	ai_chat_tool_create_read_page,
	ai_chat_tool_create_glob_pages,
	ai_chat_tool_create_grep_pages,
	ai_chat_tool_create_text_search_pages,
	ai_chat_tool_create_write_page,
	ai_chat_tool_create_edit_page,
} from "../server/server-ai-tools.ts";
import app_convex_schema from "./schema.ts";
import type { RouterForConvexModules } from "./http.ts";
import { Result } from "../src/lib/errors-as-values-utils.ts";

export const threads_list = query({
	args: {
		paginationOpts: paginationOptsValidator,
		archived: v.optional(v.boolean()),
	},
	handler: async (ctx, args) => {
		const numItems = math_clamp(args.paginationOpts.numItems ?? 100, 1, 100);
		const archived = args.archived ?? false;

		const threads_query = ctx.db
			.query("ai_chat_threads")
			.withIndex("by_workspace_project_archived_last_message_at", (q) =>
				q
					.eq("workspaceId", ai_chat_HARDCODED_ORG_ID)
					.eq("projectId", ai_chat_HARDCODED_PROJECT_ID)
					.eq("archived", archived),
			);

		const result = await threads_query.order("desc").paginate({
			...args.paginationOpts,
			numItems,
		});

		return result;
	},
});

/**
 * Query to get a single thread by ID
 */
export const thread_get = query({
	args: {
		/**
		 * Can be a temporary ID generated by Assistant UI
		 **/
		threadId: v.string(),
	},
	handler: async (ctx, args) => {
		const id_normalized = ctx.db.normalizeId("ai_chat_threads", args.threadId);

		if (!id_normalized) {
			return null;
		}

		const thread = await ctx.db.get("ai_chat_threads", id_normalized);

		// Verify the thread belongs to the current workspace
		if (thread && thread.workspaceId !== ai_chat_HARDCODED_ORG_ID) {
			return null;
		}

		return thread;
	},
});

/**
 * Mutation to create a new thread
 */
export const thread_create = mutation({
	args: v.object({
		clientGeneratedId: app_convex_schema.tables.ai_chat_threads.validator.fields.clientGeneratedId,
		title: v.optional(app_convex_schema.tables.ai_chat_threads.validator.fields.title),
		lastMessageAt: app_convex_schema.tables.ai_chat_threads.validator.fields.lastMessageAt,
	}),
	handler: async (ctx, args) => {
		const user = await server_convex_get_user_fallback_to_anonymous(ctx);

		const now = Date.now();

		const thread_id = await ctx.db.insert("ai_chat_threads", {
			workspaceId: ai_chat_HARDCODED_ORG_ID,
			projectId: ai_chat_HARDCODED_PROJECT_ID,
			clientGeneratedId: args.clientGeneratedId,
			title: args.title ?? null,
			lastMessageAt: args.lastMessageAt,
			archived: false,
			runtime: "aisdk_5",
			createdBy: user.id,
			updatedBy: user.id,
			updatedAt: now,
			starred: false,
		});

		return {
			thread_id,
		};
	},
});

export const thread_branch = mutation({
	args: {
		threadId: v.string(),
		messageId: v.optional(v.string()),
	},
	returns: v.object({
		threadId: v.id("ai_chat_threads"),
	}),
	handler: async (ctx, args) => {
		const sourceThreadId = ctx.db.normalizeId("ai_chat_threads", args.threadId);
		if (!sourceThreadId) {
			throw should_never_happen("[thread_branch] Invalid thread ID", { threadId: args.threadId });
		}

		const sourceThread = await ctx.db.get("ai_chat_threads", sourceThreadId);
		if (!sourceThread) {
			throw should_never_happen("[thread_branch] Missing thread", { threadId: args.threadId });
		}

		const user = await server_convex_get_user_fallback_to_anonymous(ctx);
		const now = Date.now();

		const sourceTitle = (sourceThread.title || "New Chat").trim() || "New Chat";
		const baseTitle = sourceTitle.replace(/ \(\d+\)$/, "");

		const unarchivedThreads = await ctx.db
			.query("ai_chat_threads")
			.withIndex("by_workspace_project_archived_last_message_at", (q) =>
				q
					.eq("workspaceId", ai_chat_HARDCODED_ORG_ID)
					.eq("projectId", ai_chat_HARDCODED_PROJECT_ID)
					.eq("archived", false),
			)
			.collect();

		const archivedThreads = await ctx.db
			.query("ai_chat_threads")
			.withIndex("by_workspace_project_archived_last_message_at", (q) =>
				q
					.eq("workspaceId", ai_chat_HARDCODED_ORG_ID)
					.eq("projectId", ai_chat_HARDCODED_PROJECT_ID)
					.eq("archived", true),
			)
			.collect();

		let maxSuffix = 0;
		for (const thread of [...unarchivedThreads, ...archivedThreads]) {
			const title = (thread.title || "New Chat").trim() || "New Chat";
			const normalized = title.replace(/ \(\d+\)$/, "");
			if (normalized !== baseTitle) {
				continue;
			}

			const match = title.match(/ \((\d+)\)$/);
			if (!match) {
				continue;
			}

			const n = Number(match[1]);
			if (Number.isFinite(n) && n > maxSuffix) {
				maxSuffix = n;
			}
		}

		const title = `${baseTitle} (${maxSuffix + 1})`;
		const clientGeneratedId = get_id_generator("ai_thread")();

		const newThreadId = await ctx.db.insert("ai_chat_threads", {
			workspaceId: ai_chat_HARDCODED_ORG_ID,
			projectId: ai_chat_HARDCODED_PROJECT_ID,
			clientGeneratedId,
			title,
			lastMessageAt: now,
			archived: false,
			runtime: "aisdk_5",
			createdBy: user.id,
			updatedBy: user.id,
			updatedAt: now,
			starred: false,
		});

		const allMessages = await ctx.db
			.query("ai_chat_threads_messages_aisdk_5")
			.withIndex("by_workspace_project_thread", (q) =>
				q
					.eq("workspaceId", ai_chat_HARDCODED_ORG_ID)
					.eq("projectId", ai_chat_HARDCODED_PROJECT_ID)
					.eq("threadId", sourceThreadId),
			)
			.collect();

		const byId = new Map<string, app_convex_Doc<"ai_chat_threads_messages_aisdk_5">>(
			allMessages.map((m) => [m._id, m]),
		);

		const newestMessage = ((/* iife */) => {
			if (args.messageId) {
				const normalized = ctx.db.normalizeId("ai_chat_threads_messages_aisdk_5", args.messageId);
				if (!normalized) {
					throw should_never_happen("[thread_branch] Invalid message ID", {
						messageId: args.messageId,
						threadId: args.threadId,
					});
				}
				const message = byId.get(normalized);
				if (!message) {
					throw should_never_happen("[thread_branch] Missing message", {
						messageId: args.messageId,
						threadId: args.threadId,
					});
				}
				return message;
			}

			let newest: app_convex_Doc<"ai_chat_threads_messages_aisdk_5"> | null = null;
			for (const message of allMessages) {
				if (!newest || message._creationTime > newest._creationTime) {
					newest = message;
				}
			}
			return newest;
		})();

		if (!newestMessage) {
			return { threadId: newThreadId };
		}

		const chain: Array<app_convex_Doc<"ai_chat_threads_messages_aisdk_5">> = [];

		let current: app_convex_Doc<"ai_chat_threads_messages_aisdk_5"> | undefined = newestMessage;
		while (current) {
			chain.push(current);
			current = current.parentId ? byId.get(current.parentId) : undefined;
		}

		chain.reverse();

		const messages: Array<{
			clientGeneratedMessageId: string;
			content: Record<string, unknown>;
		}> = [];

		for (const msg of chain) {
			const content = msg.content as unknown as ai_chat_AiSdk5UiMessage;
			const nextId = get_id_generator("ai_message")();
			const metadata = content.metadata
				? omit_properties(content.metadata as Record<string, unknown>, ["convexParentId", "convexId"])
				: undefined;

			messages.push({
				clientGeneratedMessageId: nextId,
				content: {
					...content,
					id: nextId,
					...(metadata ? { metadata } : {}),
				},
			});
		}

		let nextParentId: app_convex_Id<"ai_chat_threads_messages_aisdk_5"> | null = null;
		for (const message of messages) {
			const insertedId: app_convex_Id<"ai_chat_threads_messages_aisdk_5"> = await ctx.db.insert(
				"ai_chat_threads_messages_aisdk_5",
				{
					workspaceId: ai_chat_HARDCODED_ORG_ID,
					projectId: ai_chat_HARDCODED_PROJECT_ID,
					parentId: nextParentId,
					threadId: newThreadId,
					createdBy: user.id,
					updatedAt: now,
					clientGeneratedMessageId: message.clientGeneratedMessageId,
					content: message.content,
				},
			);

			nextParentId = insertedId;
		}

		await ctx.db.patch("ai_chat_threads", newThreadId, {
			lastMessageAt: now,
			updatedAt: now,
			updatedBy: user.id,
		});

		return { threadId: newThreadId };
	},
});

/**
 * Mutation to update thread details
 */
export const thread_update = mutation({
	args: {
		threadId: v.string(),
		title: v.optional(v.union(v.string(), v.null())),
		isArchived: v.optional(v.boolean()),
		starred: v.optional(v.boolean()),
	},
	handler: async (ctx, args) => {
		const threadId = ctx.db.normalizeId("ai_chat_threads", args.threadId);
		if (!threadId) {
			return Result({ _nay: { message: "Invalid thread ID" } });
		}

		const user = await server_convex_get_user_fallback_to_anonymous(ctx);

		await ctx.db.patch(
			"ai_chat_threads",
			threadId,
			Object.assign(
				{
					updatedBy: user.id,
					updatedAt: Date.now(),
				},
				args.title !== undefined
					? {
							title: args.title,
						}
					: {},
				args.isArchived !== undefined
					? {
							archived: args.isArchived,
						}
					: {},
				args.starred !== undefined
					? {
							starred: args.starred,
						}
					: {},
			),
		);
	},
});

/**
 * Mutation to archive/unarchive a thread
 */
export const thread_archive = mutation({
	args: {
		threadId: v.id("ai_chat_threads"),
	},
	handler: async (ctx, args) => {
		const user = await server_convex_get_user_fallback_to_anonymous(ctx);

		const now = Date.now();

		await ctx.db.patch("ai_chat_threads", args.threadId, {
			archived: true,
			updatedBy: user.id,
			updatedAt: now,
		});
	},
});

/**
 * Query to list messages in a thread
 */
export const thread_messages_list = query({
	args: {
		threadId: v.string(),
		order: v.optional(v.union(v.literal("asc"), v.literal("desc"))),
	},
	handler: async (ctx, args) => {
		const threadId = ctx.db.normalizeId("ai_chat_threads", args.threadId);
		if (!threadId) {
			return null;
		}

		const messages = await ctx.db
			.query("ai_chat_threads_messages_aisdk_5")
			.withIndex("by_workspace_project_thread", (q) =>
				q
					.eq("workspaceId", ai_chat_HARDCODED_ORG_ID)
					.eq("projectId", ai_chat_HARDCODED_PROJECT_ID)
					.eq("threadId", threadId),
			)
			.order(args.order ?? "desc")
			.collect();

		return { messages };
	},
});

/**
 * Mutation to add one or more messages to a thread.
 */
export const thread_messages_add = mutation({
	args: {
		threadId: v.id("ai_chat_threads"),
		parentId: v.optional(v.union(v.string(), v.null())),
		messages: v.array(
			v.object({
				clientGeneratedMessageId:
					app_convex_schema.tables.ai_chat_threads_messages_aisdk_5.validator.fields.clientGeneratedMessageId,
				content: app_convex_schema.tables.ai_chat_threads_messages_aisdk_5.validator.fields.content,
			}),
		),
	},
	returns: v.object({
		ids: v.array(v.id("ai_chat_threads_messages_aisdk_5")),
	}),
	handler: async (ctx, args) => {
		const parentId = args.parentId ? ctx.db.normalizeId("ai_chat_threads_messages_aisdk_5", args.parentId) : null;

		const now = Date.now();

		const user = await server_convex_get_user_fallback_to_anonymous(ctx);

		const ids: Array<app_convex_Id<"ai_chat_threads_messages_aisdk_5">> = [];

		let nextParentId = parentId;
		for (const message of args.messages) {
			const messageId = await ctx.db.insert("ai_chat_threads_messages_aisdk_5", {
				workspaceId: ai_chat_HARDCODED_ORG_ID,
				projectId: ai_chat_HARDCODED_PROJECT_ID,
				parentId: nextParentId,
				threadId: args.threadId,
				createdBy: user.id,
				updatedAt: now,
				clientGeneratedMessageId: message.clientGeneratedMessageId,
				content: message.content,
			});

			ids.push(messageId);
			nextParentId = messageId;
		}

		if (ids.length > 0) {
			await ctx.db.patch("ai_chat_threads", args.threadId, {
				lastMessageAt: now,
				updatedAt: now,
				updatedBy: user.id,
			});
		}

		return { ids };
	},
});

export const upsert_ai_pending_edit = internalMutation({
	args: {
		workspaceId: v.string(),
		projectId: v.string(),
		pageId: v.id("pages"),
		baseContent: v.string(),
		modifiedContent: v.string(),
	},
	returns: v.null(),
	handler: async (ctx, args) => {
		const user = await server_convex_get_user_fallback_to_anonymous(ctx);
		const now = Date.now();

		const pendingEdits = await ctx.db
			.query("ai_chat_pending_edits")
			.withIndex("by_workspace_project_user_page", (q) =>
				q
					.eq("workspaceId", args.workspaceId)
					.eq("projectId", args.projectId)
					.eq("userId", user.id)
					.eq("pageId", args.pageId),
			)
			.first();

		if (!pendingEdits) {
			await ctx.db.insert("ai_chat_pending_edits", {
				workspaceId: args.workspaceId,
				projectId: args.projectId,
				userId: user.id,
				pageId: args.pageId,
				baseContent: args.baseContent,
				modifiedContent: args.modifiedContent,
				updatedAt: now,
			});
		} else {
			await ctx.db.patch("ai_chat_pending_edits", pendingEdits._id, {
				modifiedContent: args.modifiedContent,
				updatedAt: now,
			});
		}

		return null;
	},
});

export const clear_ai_pending_edit = mutation({
	args: {
		workspaceId: v.string(),
		projectId: v.string(),
		pageId: v.id("pages"),
		expectedUpdatedAt: v.optional(v.number()),
	},
	returns: v.null(),
	handler: async (ctx, args) => {
		const user = await server_convex_get_user_fallback_to_anonymous(ctx);

		const pendingEdits = await ctx.db
			.query("ai_chat_pending_edits")
			.withIndex("by_workspace_project_user_page", (q) =>
				q
					.eq("workspaceId", args.workspaceId)
					.eq("projectId", args.projectId)
					.eq("userId", user.id)
					.eq("pageId", args.pageId),
			)
			.first();

		if (!pendingEdits) {
			return null;
		}

		if (args.expectedUpdatedAt != null && pendingEdits.updatedAt !== args.expectedUpdatedAt) {
			return null;
		}

		await ctx.db.delete("ai_chat_pending_edits", pendingEdits._id);
		return null;
	},
});

export const get_ai_pending_edit = query({
	args: {
		workspaceId: v.string(),
		projectId: v.string(),
		pageId: v.id("pages"),
	},
	returns: v.union(doc(app_convex_schema, "ai_chat_pending_edits"), v.null()),
	handler: async (ctx, args) => {
		const user = await server_convex_get_user_fallback_to_anonymous(ctx);

		const pendingEdits = await ctx.db
			.query("ai_chat_pending_edits")
			.withIndex("by_workspace_project_user_page", (q) =>
				q
					.eq("workspaceId", args.workspaceId)
					.eq("projectId", args.projectId)
					.eq("userId", user.id)
					.eq("pageId", args.pageId),
			)
			.first();

		return pendingEdits;
	},
});

export function ai_chat_http_routes(router: RouterForConvexModules) {
	return {
		...((/* iife */ path = "/api/chat" as const satisfies api_schemas_Main_Path) => ({
			[path]: {
				...((/* iife */ method = "POST" as const satisfies RouteSpec["method"]) => ({
					[method]: ((/* iife */) => {
						/**
						 * See {@link PrepareSendMessagesRequest}.
						 *
						 * See {@link AssistantChatTransport.prepareSendMessagesRequest}.
						 **/
						const bodyValidator = z.object({
							/**
							 * The messages to append to the thread.
							 */
							messages: z.array(z.any()),
							trigger: z.enum(["submit-message", "regenerate-message"]),
							/**
							 * The id of the message to which the new message should be appended.
							 * `null` means root.
							 */
							parentId: z.string().nullable().optional(),
							/**
							 * The id of the thread to which the new message should be appended.
							 *
							 * `undefined` for new threads.
							 */
							threadId: z.string().optional(),

							/**
							 * The client generated id for a new thread.
							 */
							clientGeneratedThreadId: z.string().optional(),
						});

						type SearchParams = never;
						type PathParams = never;
						type Headers = Record<string, string>;
						type Body = z.infer<typeof bodyValidator>;

						const handler = async (ctx: ActionCtx, request: Request) => {
							try {
								const requestParseResult = await server_request_json_parse_and_validate(request, bodyValidator);

								if (requestParseResult._nay) {
									return {
										status: 400,
										body: requestParseResult._nay,
									} as const;
								}

								if (
									requestParseResult._yay.threadId == null &&
									requestParseResult._yay.clientGeneratedThreadId == null
								) {
									return {
										status: 400,
										body: {
											message: "One of `threadId` or `clientGeneratedThreadId` is required",
										},
									} as const;
								}

								// Validate the messages if they are present
								if (requestParseResult._yay.messages.length > 0) {
									try {
										await validateUIMessages({
											messages: requestParseResult._yay.messages,
											tools: undefined,
										});
									} catch (error) {
										if (error instanceof TypeValidationError) {
											return {
												status: 400,
												body: {
													message: "Invalid messages format",
												},
											} as const;
										} else {
											const msg = "Failed to validate chat messages";
											should_never_happen(msg, {
												error: error,
											});
											return {
												status: 500,
												body: {
													message: msg,
												},
											} as const;
										}
									}
								}

								const now = Date.now();

								const body = requestParseResult._yay;

								let threadId = null;
								let createdThreadId = null;

								const requestMessages = body.messages as ai_chat_AiSdk5UiMessage[];
								let uiMessages: ai_chat_AiSdk5UiMessage[] = [];

								if (body.threadId) {
									const existingThread = await ctx.runQuery(api.ai_chat.thread_get, {
										threadId: body.threadId,
									});
									if (!existingThread) {
										return {
											status: 400,
											body: {
												message: "Thread not found",
											},
										} as const;
									}

									threadId = existingThread._id;
								} else {
									if (!body.clientGeneratedThreadId) {
										throw should_never_happen(
											"`body.clientGeneratedThreadId` missing, the request was not properly validated at the top of this handler",
											{
												threadId,
												clientGeneratedThreadId: body.clientGeneratedThreadId,
											},
										);
									}

									const created = await ctx.runMutation(api.ai_chat.thread_create, {
										// Store the optimistic client thread id on the persisted thread.
										// This lets the frontend dedupe the optimistic entry as soon as the
										// thread appears in `threads_list`, even if the SSE `data-thread-id`
										// mapping arrives slightly later.
										clientGeneratedId: body.clientGeneratedThreadId ?? get_id_generator("ai_thread")(),
										lastMessageAt: now,
									});

									createdThreadId = threadId = created.thread_id;
								}

								// FIX(parentId-race-condition): Track the resolved Convex doc ID for `onFinish` persistence.
								let resolvedParentId: string | null | undefined = body.parentId;

								if (threadId) {
									do {
										const threadMessagesResult = await ctx.runQuery(api.ai_chat.thread_messages_list, {
											threadId: threadId as app_convex_Id<"ai_chat_threads">,
											order: "asc",
										});

										if (!threadMessagesResult) {
											break;
										}

										// FIX(parentId-race-condition): Index by both Convex doc `_id` and `clientGeneratedMessageId`
										// so lookups work regardless of which ID format the client sends.
										// The client may send a client-generated ID as `body.parentId` when the Convex
										// real-time subscription hasn't delivered the persisted messages yet.
										//
										// BEFORE:
										// const messagesMap = new Map<string, app_convex_Doc<"ai_chat_threads_messages_aisdk_5">>(
										// 	threadMessagesResult.messages.map((msg) => [msg._id, msg]),
										// );
										const messagesMap = new Map<string, app_convex_Doc<"ai_chat_threads_messages_aisdk_5">>();
										for (const msg of threadMessagesResult.messages) {
											messagesMap.set(msg._id, msg);
											if (msg.clientGeneratedMessageId) {
												messagesMap.set(msg.clientGeneratedMessageId, msg);
											}
										}

										const reconstructedMessages: app_convex_Doc<"ai_chat_threads_messages_aisdk_5">[] = [];

										let nextMessageId = body.parentId;
										while (nextMessageId) {
											const message = messagesMap.get(nextMessageId);
											if (!message) {
												throw should_never_happen("Failed to reconstruct messages", {
													threadId,
													messageId: nextMessageId,
												});
											}

											reconstructedMessages.push(message);
											nextMessageId = message.parentId as string;
										}

										// FIX(parentId-race-condition): Resolve `body.parentId` to the Convex doc `_id` so that
										// `onFinish` can persist the parent chain with real doc IDs.
										// Without this, `normalizeId()` in `thread_messages_add` silently returns `null`
										// for client-generated IDs, breaking the parent chain.
										if (body.parentId) {
											const parentMsg = messagesMap.get(body.parentId);
											if (parentMsg) {
												resolvedParentId = parentMsg._id;
											}
										}

										uiMessages = reconstructedMessages.reverse().map(
											(msg) =>
												({
													...(msg.content as any),
													id: msg._id,
												}) as ai_chat_AiSdk5UiMessage,
										);
									} while (0);
								}

								// Persist user-submitted messages before starting assistant streaming.
								// This keeps edits durable even when the user stops generation.
								if (requestMessages.length > 0) {
									const requestMessagesToAdd = requestMessages.map((message) => ({
										clientGeneratedMessageId: message.id,
										content: message,
									}));

									const persistedRequestMessages = await ctx.runMutation(api.ai_chat.thread_messages_add, {
										threadId: threadId as app_convex_Id<"ai_chat_threads">,
										parentId: resolvedParentId,
										messages: requestMessagesToAdd,
									});

									uiMessages.push(
										...requestMessages.map((requestMessage, index) => {
											const persistedMessageId = persistedRequestMessages.ids[index];
											if (!persistedMessageId) {
												throw should_never_happen("Failed to map request message to persisted message ID", {
													threadId,
													requestMessageId: requestMessage.id,
													index,
												});
											}

											return {
												...requestMessage,
												id: persistedMessageId,
											};
										}),
									);

									resolvedParentId = persistedRequestMessages.ids.at(-1) ?? resolvedParentId;
								}

								const tools = {
									weather: tool({
										description: "Get the weather in a location (in Celsius)",
										inputSchema: z.object({
											location: z.string().describe("The location to get the weather for"),
										}),
										execute: async ({ location }) => ({
											location,
											temperature: "200Â°",
										}),
									}),
									read_page: ai_chat_tool_create_read_page(ctx),
									list_pages: ai_chat_tool_create_list_pages(ctx),
									glob_pages: ai_chat_tool_create_glob_pages(ctx),
									grep_pages: ai_chat_tool_create_grep_pages(ctx),
									text_search_pages: ai_chat_tool_create_text_search_pages(ctx),
									write_page: ai_chat_tool_create_write_page(ctx),
									edit_page: ai_chat_tool_create_edit_page(ctx),
								} as const;

								const modelMessages = convertToModelMessages(uiMessages);

								const stream = createUIMessageStream<ai_chat_AiSdk5UiMessage>({
									generateId: get_id_generator("ai_message"),
									execute: async ({ writer }) => {
										// TODO(ai-chat): If we allocate Convex message docs up front, emit a transient `data-message-ids`
										// part here (while `writer` is available) so the client can swap optimistic UIMessage ids to
										// Convex ids and/or drop optimistic messages immediately, without persisting client ids in DB.
										if (createdThreadId) {
											writer.write({
												type: "data-thread-id",
												data: {
													threadId: createdThreadId,
												},
												transient: true,
											});
										}

										writer.write({
											type: "message-metadata",
											messageMetadata: {
												convexParentId: uiMessages.at(-1)?.id,
											},
										});

										const result1 = streamText({
											model: openai("gpt-5-nano"),
											system: `Either respond directly to the user or use the tools at your disposal.`,
											messages: modelMessages,
											maxOutputTokens: 2000,
											abortSignal: request.signal,
											providerOptions: {
												openai: {
													reasoningEffort: "low",
												},
											},
											toolChoice: "auto",
											stopWhen: stepCountIs(5),
											tools,
										});

										writer.merge(result1.toUIMessageStream());

										if (request.signal.aborted) {
											return;
										}

										const response1 = await result1.response;

										if (request.signal.aborted) {
											return;
										}

										const thread = await ctx.runQuery(api.ai_chat.thread_get, { threadId });
										const existingTitle = typeof thread?.title === "string" ? thread.title.trim() : "";

										if (thread && !existingTitle) {
											try {
												if (request.signal.aborted) {
													return;
												}

												const titleMessages = [...modelMessages, ...response1.messages];
												const titleResult = streamText({
													model: openai("gpt-4.1-nano"),
													system:
														"Generate a concise, descriptive title (max 6 words) for this conversation.\n" +
														"The title should capture the main topic or purpose.\n" +
														"Respond with ONLY the title, no quotes or extra text.",
													messages: titleMessages,
													stopWhen: stepCountIs(1),
													temperature: 0.3,
													maxOutputTokens: 50,
													abortSignal: request.signal,
												});

												const reader = titleResult.textStream.getReader();
												let title = "";
												while (true) {
													const { value, done } = await reader.read();
													if (done) {
														break;
													}

													if (value) {
														title += value;
													}
												}

												const trimmedTitle = title.trim();
												if (trimmedTitle) {
													writer.write({
														type: "data-chat-title",
														data: { title: trimmedTitle },
														transient: true,
													});

													await ctx.runMutation(api.ai_chat.thread_update, {
														threadId: thread._id,
														title: trimmedTitle,
													});
												}
											} catch (error: unknown) {
												console.error("Title generation error:", error);
											}
										}
									},
									onError: (error: unknown) => {
										console.error("AI chat stream error:", error);
										return error instanceof Error ? error.message : String(error);
									},
									onFinish: async (result) => {
										try {
											if (request.signal.aborted || !result.responseMessage) {
												return;
											}

											const messagesToAdd = [
												{
													clientGeneratedMessageId: result.responseMessage.id,
													content: result.responseMessage,
												},
											];

											// Persist the assistant response below the last persisted request message.
											// This keeps user edits durable even when generation was previously stopped.
											await ctx.runMutation(api.ai_chat.thread_messages_add, {
												threadId: threadId as app_convex_Id<"ai_chat_threads">,
												parentId: resolvedParentId,
												messages: messagesToAdd,
											});
										} catch (error: unknown) {
											console.error("Failed to persist chat messages:", error);
										}
									},
								});

								return {
									status: 200,
									body: stream,
								} as const;
							} catch (error: unknown) {
								console.error("AI chat stream error:", error);

								if (error instanceof Error) {
									return {
										status: 500,
										body: {
											message: error.message,
										},
									} as const;
								}

								return {
									status: 500,
									body: {
										message: "Internal server error",
									},
								} as const;
							}
						};

						router.route({
							path,
							method,
							handler: httpAction(async (ctx, request) => {
								const result = await handler(ctx, request);

								if (result.status === 200) {
									return createUIMessageStreamResponse({
										status: result.status,
										stream: result.body,
										consumeSseStream: consumeStream,
									});
								}

								return Response.json(result.body, result);
							}),
						});

						return {} as {
							pathParams: PathParams;
							searchParams: SearchParams;
							headers: Headers;
							body: Body;
							response: api_schemas_BuildResponseSpecFromHandler<typeof handler>;
						};
					})(),
				}))(),
			},
		}))(),

		...((/* iife */ path = "/api/v1/runs/stream" as const satisfies api_schemas_Main_Path) => ({
			[path]: {
				...((/* iife */ method = "POST" as const satisfies RouteSpec["method"]) => ({
					[method]: ((/* iife */) => {
						/**
						 * See {@link PrepareSendMessagesRequest}.
						 *
						 * See {@link AssistantChatTransport.prepareSendMessagesRequest}.
						 **/
						const bodyValidator = z.object({
							thread_id: z.string(),
							assistant_id: z.string(),
							messages: z.array(z.any()),
							response_format: z.string().optional(),
						});

						type SearchParams = never;
						type PathParams = never;
						type Headers = Record<string, string>;
						type Body = z.infer<typeof bodyValidator>;

						const handler = async (ctx: ActionCtx, request: Request) => {
							try {
								const body = await request.json();

								if (body.assistant_id !== "system/thread_title") {
									return {
										status: 400,
										body: {
											message: "Invalid stream ID",
										},
									} as const;
								}

								const messages = body.messages || [];
								const thread_id = body.thread_id;

								// Extract conversation text from messages for title generation
								const conversation_text = messages
									.map((msg: any) =>
										[
											`${msg.role}:`,
											Array.isArray(msg.content) ? msg.content.map((part: any) => part.text).join(" ") : msg.content,
										]
											.filter(Boolean)
											.join(" "),
									)
									.filter(Boolean)
									.join("\n");

								// Generate title using AI with streaming
								const result = streamText({
									model: openai("gpt-4.1-nano"),
									system: `Generate a concise, descriptive title (max 6 words) for this conversation.
										The title should capture the main topic or purpose.
										Respond with ONLY the title, no quotes or extra text.`,
									messages: [
										{
											role: "user",
											content: `Generate a title for this conversation:\n\n${conversation_text}`,
										},
									],
									stopWhen: stepCountIs(1),
									temperature: 0.3,
									maxOutputTokens: 50,
									experimental_transform: smoothStream({
										delayInMs: 100,
									}),
								});

								// Transform the AI stream to properly encode text chunks
								let title = "";

								// Trigger mutation when the stream is finished
								const transform_stream = new TransformStream({
									transform(chunk, controller) {
										title += chunk;
										controller.enqueue(chunk);
									},
									flush: async () => {
										await ctx.runMutation(api.ai_chat.thread_update, {
											threadId: thread_id,
											title,
										});
									},
								});

								// Pipe the AI textStream through the transformer, insprired by ai-sdk's `createTextStreamResponse`
								const stream = result.textStream.pipeThrough(transform_stream).pipeThrough(new TextEncoderStream());

								void result.consumeStream();

								return {
									status: 200,
									body: stream,
								} as const;
							} catch (error: unknown) {
								console.error("Title generation error:", error);

								return {
									status: 500,
									body: {
										message: error instanceof Error ? error.message : "Unknown error",
									},
								} as const;
							}
						};

						router.route({
							path,
							method,
							handler: httpAction(async (ctx, request) => {
								const result = await handler(ctx, request);

								if (result.status === 200) {
									return new Response(result.body, {
										status: result.status,
									});
								}

								return Response.json(result, result);
							}),
						});

						return {} as {
							pathParams: PathParams;
							searchParams: SearchParams;
							headers: Headers;
							body: Body;
							response: api_schemas_BuildResponseSpecFromHandler<typeof handler>;
						};
					})(),
				}))(),
			},
		}))(),
	};
}
