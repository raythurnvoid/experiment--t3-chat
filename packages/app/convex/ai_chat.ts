import {
	ai_chat_HARDCODED_PROJECT_ID,
	ai_chat_HARDCODED_ORG_ID,
	type ai_chat_MessageContentPartText,
	type ai_chat_MessageContentPartToolCall,
} from "../src/lib/ai-chat.ts";
import { math_clamp } from "../src/lib/utils.ts";
import { query, mutation, httpAction } from "./_generated/server";
import { api } from "./_generated/api";
import { paginationOptsValidator } from "convex/server";
import { v } from "convex/values";
import app_convex_schema from "./schema.ts";
import { openai } from "@ai-sdk/openai";
import { streamText, tool, smoothStream, formatDataStreamPart, type CoreMessage, createDataStreamResponse } from "ai";
import { z } from "zod";
import { createArtifactArgsSchema } from "../src/types/artifact-schemas";
import type { api_schemas_Main } from "../src/lib/api-schemas.ts";
import {
	server_convex_headers_cors,
	server_convex_get_user_fallback_to_anonymous,
	server_convex_response_error,
} from "./lib/server_utils.ts";
import type { app_convex_Doc, app_convex_Id } from "../src/lib/app-convex-client.ts";
import {
	ai_tool_create_list_pages,
	ai_tool_create_read_page,
	ai_tool_create_glob_page,
} from "./lib/server_ai_tools.ts";

// Removed opencode-based read tool; using DB-backed tool from server_ai_tools

/**
 * Query to list all threads for a workspace with pagination
 */
export const threads_list = query({
	args: {
		pagination_opts: paginationOptsValidator,
		include_archived: v.optional(v.boolean()),
	},
	handler: async (ctx, args) => {
		args.pagination_opts.numItems = math_clamp(args.pagination_opts.numItems, 1, 50);

		let threads_query = ctx.db
			.query("threads")
			.withIndex("by_workspace", (q) => q.eq("workspace_id", ai_chat_HARDCODED_ORG_ID));

		if (args.include_archived !== true) {
			threads_query = threads_query.filter((q) => q.eq(q.field("archived"), false));
		}

		const result = await threads_query.order("desc").paginate(args.pagination_opts);

		return {
			...result,
			page: {
				threads: result.page,
			},
		};
	},
});

/**
 * Query to get a single thread by ID
 */
export const thread_get = query({
	args: {
		/**
		 * Can be a temporary ID generated by Assistant UI
		 **/
		thread_id: v.string(),
	},
	handler: async (ctx, args) => {
		const id_normalized = ctx.db.normalizeId("threads", args.thread_id);

		if (!id_normalized) {
			return null;
		}

		const thread = await ctx.db.get(id_normalized);

		// Verify the thread belongs to the current workspace
		if (thread && thread.workspace_id !== ai_chat_HARDCODED_ORG_ID) {
			return null;
		}

		return thread;
	},
});

/**
 * Mutation to create a new thread
 */
export const thread_create = mutation({
	args: {
		title: v.optional(v.string()),
		last_message_at: v.number(), // timestamp in milliseconds
		metadata: v.optional(v.any()),
		external_id: v.optional(v.union(v.string())),
	},
	handler: async (ctx, args) => {
		const created_by = await server_convex_get_user_fallback_to_anonymous(ctx);

		const now = Date.now();

		const thread_id = await ctx.db.insert("threads", {
			title: args.title ?? "New Chat",
			last_message_at: args.last_message_at,
			archived: false,
			workspace_id: ai_chat_HARDCODED_ORG_ID,
			created_by: created_by.name,
			updated_by: created_by.name,
			updated_at: now,
			external_id: args.external_id ?? null,
			project_id: ai_chat_HARDCODED_PROJECT_ID,
			starred: false,
		});

		return {
			thread_id,
		};
	},
});

/**
 * Mutation to update thread details
 */
export const thread_update = mutation({
	args: {
		thread_id: v.id("threads"),
		title: v.optional(v.string()),
		is_archived: v.optional(v.boolean()),
		starred: v.optional(v.boolean()),
	},
	handler: async (ctx, args) => {
		const updated_by = await server_convex_get_user_fallback_to_anonymous(ctx);

		await ctx.db.patch(
			args.thread_id,
			Object.assign(
				{
					updated_by: updated_by.name,
					updated_at: Date.now(),
				},
				args.title !== undefined
					? {
							title: args.title,
						}
					: {},
				args.is_archived !== undefined
					? {
							archived: args.is_archived,
						}
					: {},
				args.starred !== undefined
					? {
							starred: args.starred,
						}
					: {},
			),
		);
	},
});

/**
 * Mutation to archive/unarchive a thread
 */
export const thread_archive = mutation({
	args: {
		thread_id: v.id("threads"),
	},
	handler: async (ctx, args) => {
		const updated_by = await server_convex_get_user_fallback_to_anonymous(ctx);

		const now = Date.now();

		await ctx.db.patch(args.thread_id, {
			archived: true,
			updated_by: updated_by.name,
			updated_at: now,
		});
	},
});

/**
 * Query to list messages in a thread
 */
export const thread_messages_list = query({
	args: {
		thread_id: v.string(),
		order: v.optional(v.union(v.literal("asc"), v.literal("desc"))),
	},
	handler: async (ctx, args) => {
		const thread_id_normalized = ctx.db.normalizeId("threads", args.thread_id);

		if (!thread_id_normalized) {
			return null;
		}

		const messages = await ctx.db
			.query("messages")
			.withIndex("by_thread", (q) => q.eq("thread_id", thread_id_normalized))
			.order(args.order ?? "desc")
			.collect();

		return { messages };
	},
});

/**
 * Mutation to add a message to a thread
 */
export const thread_messages_add = mutation({
	args: {
		thread_id: v.id("threads"),
		parent_id: v.union(v.id("messages"), v.null()),
		format: v.string(),
		content: app_convex_schema.tables.messages.validator.fields.content,
	},
	handler: async (ctx, args) => {
		const created_by = await server_convex_get_user_fallback_to_anonymous(ctx);

		const now = Date.now();

		// Insert the message
		const message_id = await ctx.db.insert("messages", {
			parent_id: args.parent_id,
			thread_id: args.thread_id,
			created_by: created_by.name,
			updated_by: created_by.name,
			created_at: now,
			updated_at: now,
			format: args.format,
			height: 1,
			content: args.content,
		});

		// Update the thread's lastMessageAt timestamp
		try {
			await ctx.db.patch(args.thread_id, {
				last_message_at: now,
				updated_at: now,
				updated_by: created_by.name,
			});
		} catch (error) {
			console.error("Failed to update thread when adding message", error);
		}

		return { message_id };
	},
});

/**
 * HTTP Action for AI chat streaming with tools
 */
export const chat = httpAction(async (ctx, request) => {
	try {
		const body = (await request.json()) as api_schemas_Main["/api/chat"]["get"]["body"];

		// Validate messages from request
		if (!Array.isArray(body.messages)) {
			return server_convex_response_error({
				message: "Invalid messages format",
				status: 400,
			});
		}

		const messages = body.thread_id
			? await (async (/* iife */) => {
					const thread_messages_result = await ctx.runQuery(api.ai_chat.thread_messages_list, {
						thread_id: body.thread_id as app_convex_Id<"threads">,
						order: "asc",
					});

					return thread_messages_result
						? to_language_model_messages(thread_messages_result.messages, {
								unstable_includeId: true,
							})
						: [];
				})()
			: [...(body.messages as CoreMessage[])];

		console.log("messages", messages);

		const response = createDataStreamResponse({
			execute: async (dataStream) => {
				const result1 = streamText({
					model: openai("gpt-4o-mini"),
					system:
						`Either respond directly to the user or use the tools at your disposal.\n` +
						"If you decide to create an artifact, do not answer and just call the tool or answer with `On it...`.\n",
					messages,
					temperature: 0.7,
					maxTokens: 2000,
					toolChoice: "auto",
					maxSteps: 2,
					tools: {
						weather: tool({
							description: "Get the weather in a location (in Celsius)",
							parameters: z.object({
								location: z.string().describe("The location to get the weather for"),
							}),
							execute: async ({ location }) => ({
								location,
								temperature: "200Â°",
							}),
						}),
						request_create_artifact: tool({
							description:
								"Request to create a text artifact that should be displayed in a separate panel.\n" +
								"Use this when the user asks for:\n" +
								"- Creating documents, articles, or stories\n" +
								"- Generating markdown content\n" +
								"- Any substantial text output that would benefit from being editable\n" +
								"- Writing essays, reports, or long-form content\n",
							parameters: z.object({}),
							execute: async () => {
								console.log("ðŸŽ¯ request_create_artifact tool called");
								return { requested: true };
							},
						}),
						read_file: ai_tool_create_read_page(ctx),
						list_pages: ai_tool_create_list_pages(ctx),
						glob_pages: ai_tool_create_glob_page(ctx),
					},
					experimental_transform: smoothStream({
						delayInMs: 100,
					}),
				});

				result1.mergeIntoDataStream(dataStream, {
					experimental_sendFinish: false,
				});

				const response1 = await result1.response;

				const should_finish = !response1.messages.some(
					(msg) =>
						msg.role === "assistant" &&
						Array.isArray(msg.content) &&
						msg.content.some(
							(content) => content.type === "tool-call" && content.toolName === "request_create_artifact",
						),
				);

				if (should_finish) {
					const finish_reason = await result1.finishReason;
					const usage = await result1.usage;
					dataStream.write(
						formatDataStreamPart("finish_message", {
							finishReason: finish_reason,
							usage,
						}),
					);
				} else {
					// Generate a simple UUID (avoiding external dependencies)
					const artifact_id = crypto.randomUUID();

					dataStream.writeData({
						type: "artifact-id",
						id: artifact_id,
					});

					const result2 = streamText({
						model: openai("gpt-4o-mini"),
						system: `Generate comprehensive, well-structured content that directly addresses what the user requested. 
							Format the content as markdown when appropriate.`,
						messages: [...messages, ...response1.messages],
						toolChoice: "required",
						temperature: 0.7,
						maxTokens: 2000,
						maxSteps: 1,
						toolCallStreaming: true,
						tools: {
							create_artifact: tool({
								description:
									"Create a text artifact that should be displayed in a separate panel " +
									"Use this when the user asks for: " +
									"- Creating documents, articles, or stories " +
									"- Generating markdown content " +
									"- Any substantial text output that would benefit from being editable " +
									"- Writing essays, reports, or long-form content",
								parameters: createArtifactArgsSchema,
								execute: async (args) => {
									console.log(`âœ… Artifact created: ${args.title}`);
									return {
										done: true,
									};
								},
							}),
						},
						experimental_transform: smoothStream({
							delayInMs: 500,
						}),
					});

					result2.mergeIntoDataStream(dataStream, {
						experimental_sendStart: false,
						experimental_sendFinish: false,
					});

					const response2 = await result2.response;

					const result3 = streamText({
						model: openai("gpt-4o-mini"),
						system: `Send a brief confirmation message to the user that the artifact has been created successfully. 
							Keep the message concise and friendly.`,
						messages: [...messages, ...response1.messages, ...response2.messages],
						temperature: 0.7,
						maxTokens: 200,
						maxSteps: 1,
						toolChoice: "none",
						experimental_transform: smoothStream({
							delayInMs: 100,
						}),
					});

					result3.mergeIntoDataStream(dataStream, {
						experimental_sendStart: false,
					});
				}
			},
			onError: (error) => {
				console.error("AI chat stream error:", error);
				return error instanceof Error ? error.message : String(error);
			},
			headers: server_convex_headers_cors(),
		});

		return response;
	} catch (error: unknown) {
		console.error("AI chat stream error:", error);

		if (error instanceof Error) {
			return server_convex_response_error({
				message: error.message,
				status: 500,
			});
		}

		return server_convex_response_error({
			message: "Internal server error",
			status: 500,
		});
	}
});

/**
 * HTTP Action for generating thread titles
 */
export const thread_generate_title = httpAction(async (ctx, request) => {
	try {
		const body = await request.json();

		if (body.assistant_id !== "system/thread_title") {
			return server_convex_response_error({
				message: "Invalid stream ID",
				status: 400,
			});
		}

		const messages = body.messages || [];
		const thread_id = body.thread_id;

		// Extract conversation text from messages for title generation
		const conversation_text = messages
			.map((msg: any) =>
				[`${msg.role}:`, Array.isArray(msg.content) ? msg.content.map((part: any) => part.text).join(" ") : msg.content]
					.filter(Boolean)
					.join(" "),
			)
			.filter(Boolean)
			.join("\n");

		// Generate title using AI with streaming
		const result = streamText({
			model: openai("gpt-4o-mini"),
			system: `Generate a concise, descriptive title (max 6 words) for this conversation.
				The title should capture the main topic or purpose.
				Respond with ONLY the title, no quotes or extra text.`,
			messages: [
				{
					role: "user",
					content: `Generate a title for this conversation:\n\n${conversation_text}`,
				},
			],
			temperature: 0.3,
			maxTokens: 50,
			experimental_transform: smoothStream({
				delayInMs: 100,
			}),
		});

		// Transform the AI stream to properly encode text chunks
		let title = "";

		// Trigger mutation when the stream is finished
		const transform_stream = new TransformStream({
			transform(chunk, controller) {
				title += chunk;
				controller.enqueue(chunk);
			},
			flush: async () => {
				await ctx.runMutation(api.ai_chat.thread_update, {
					thread_id,
					title,
				});
			},
		});

		// Pipe the AI textStream through the transformer, insprired by ai-sdk's `createDataStreamResponse`
		const stream = result.textStream.pipeThrough(transform_stream).pipeThrough(new TextEncoderStream());

		return new Response(stream, {
			headers: server_convex_headers_cors(),
		});
	} catch (error: unknown) {
		console.error("Title generation error:", error);

		return server_convex_response_error({
			message: error instanceof Error ? error.message : "Unknown error",
			status: 500,
		});
	}
});

const assistant_message_splitter = () => {
	const stash: CoreMessage[] = [];
	let assistantMessage = {
		role: "assistant" as const,
		content: [] as any[],
	};
	let toolMessage = {
		role: "tool" as const,
		content: [] as any[],
	};

	return {
		addTextMessagePart: (part: ai_chat_MessageContentPartText) => {
			if (toolMessage.content.length > 0) {
				stash.push(assistantMessage);
				stash.push(toolMessage);

				assistantMessage = {
					role: "assistant" as const,
					content: [] as any[],
				};

				toolMessage = {
					role: "tool" as const,
					content: [] as any[],
				};
			}

			assistantMessage.content.push(part);
		},
		addToolCallPart: (part: ai_chat_MessageContentPartToolCall) => {
			assistantMessage.content.push({
				type: "tool-call",
				toolCallId: part.toolCallId,
				toolName: part.toolName,
				args: part.args,
			});

			toolMessage.content.push({
				type: "tool-result",
				toolCallId: part.toolCallId,
				toolName: part.toolName,
				...("artifact" in part ? { artifact: part.artifact } : {}),
				result: part.result === undefined ? "Error: tool is has no configured code to run" : part.result,
				isError: part.isError ?? part.result === undefined,
			});
		},
		getMessages: () => {
			if (toolMessage.content.length > 0) {
				return [...stash, assistantMessage, toolMessage];
			}

			return [...stash, assistantMessage];
		},
	};
};

/**
 * Copied from [Assistant UI toLanguageModelMessages.tsx](../assistant-ui/packages/react-ai-sdk/src/converters/toLanguageModelMessages.ts)
 */
function to_language_model_messages(
	message: app_convex_Doc<"messages">[],
	options: { unstable_includeId?: boolean | undefined } = {},
): CoreMessage[] {
	const includeId = options.unstable_includeId ?? false;
	return message.flatMap(
		// @ts-expect-error Weird TS error
		(message_wrapper) => {
			const id = message_wrapper._id;
			const message = message_wrapper.content;

			const role = message.role;
			switch (role) {
				case "system": {
					return [
						{
							...(includeId ? { unstable_id: id } : {}),
							role: "system",
							content: message.content[0].text,
						},
					];
				}

				case "user": {
					const attachments = "attachments" in message ? (message.attachments as any[]) : [];
					const content = [...message.content, ...attachments.map((a) => a.content).flat()];
					const msg = {
						...(includeId ? { unstable_id: id } : {}),
						role: "user",
						content: content.map((part) => {
							const type = part.type;
							switch (type) {
								case "text": {
									return part;
								}

								case "image": {
									return {
										type: "image",
										image: new URL(part.image),
									};
								}

								case "file": {
									return {
										type: "file",
										data: new URL(part.data),
										mimeType: part.mimeType,
									};
								}

								default: {
									const unhandledType: "audio" = type;
									throw new Error(`Unspported message part type: ${unhandledType}`);
								}
							}
						}),
					};
					return [msg];
				}

				case "assistant": {
					const splitter = assistant_message_splitter();
					for (const part of message.content) {
						const type = part.type;
						switch (type) {
							case "reasoning":
							case "source":
							case "file": {
								break; // reasoning, source, and file parts are omitted
							}

							case "text": {
								splitter.addTextMessagePart(part);
								break;
							}
							case "tool-call": {
								splitter.addToolCallPart(part);
								break;
							}
							default: {
								const unhandledType: never = type;
								throw new Error(`Unhandled message part type: ${unhandledType}`);
							}
						}
					}
					return splitter.getMessages();
				}

				default: {
					const unhandledRole: never = role;
					throw new Error(`Unknown message role: ${unhandledRole}`);
				}
			}
		},
	);
}
